{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.utils import shuffle\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_num = 100\n",
    "kernel_vec = [3, 4, 5]\n",
    "epochs = 10\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are using Restaurant-large dataset.\n",
    "* Here we are loading the train and test datasets.\n",
    "* Also we are extracting sentences, aspects and sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file into a list\n",
    "with open('./acsa-restaurant-large/acsa_train.json','rb') as f:\n",
    "    data1 = json.load(f)\n",
    "\n",
    "with open('./acsa-restaurant-large/acsa_test.json','rb') as f:\n",
    "    data2 = json.load(f)\n",
    "\n",
    "sentence_data = [x['sentence'] for x in data1] + [x['sentence'] for x in data2]\n",
    "aspect_data = [x['aspect'] for x in data1] + [x['aspect'] for x in data2]\n",
    "sentiment_data = [x['sentiment'] for x in data1] + [x['sentiment'] for x in data2]\n",
    "\n",
    "sentence_data, aspect_data, sentiment_data = shuffle(sentence_data, aspect_data, sentiment_data)\n",
    "\n",
    "# print('Number of sentences: ', len(sentence_data))\n",
    "# print('Number of aspects: ', len(aspect_data))\n",
    "# print('Number of sentiments: ', len(sentiment_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the following cell, we are just making a wordcount of each word in the sentences\n",
    "* We are removing symbols at end of each word.\n",
    "* If the word occurs for the first time, we assign the count to zero\n",
    "* else we increase the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = {}\n",
    "for example in sentence_data:\n",
    "    for word in example.split():\n",
    "        if word[-1] in ['.',',','!','?']:\n",
    "            word = word[:-1]\n",
    "        if word not in data_words:\n",
    "            data_words[word] = 0\n",
    "        else:\n",
    "            data_words[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Similar to the above cell, we perform the same operations for aspect categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_categories = {}\n",
    "for example in aspect_data:\n",
    "    for word in example.split():\n",
    "        if word[-1] in ['.',',','!','?']:\n",
    "            word = word[:-1]\n",
    "        if word not in aspect_categories:\n",
    "            aspect_categories[word] = 0\n",
    "        else:\n",
    "            aspect_categories[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6959\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(data_words))\n",
    "print(len(aspect_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the following cell we are loading the glove file and encoding vectors for words present in our data.\n",
    "* `glove_word_to_vec_map:` All words in glove file and the corresponding encodings, \n",
    "* `data_word_to_vec_map:`All words in our data and the corresponding encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load glove vectors\n",
    "glove_folder = os.path.join(os.getcwd(), 'glove_file')\n",
    "\n",
    "# get path of glove.6B.300d.txt file in test folder\n",
    "glove_file = os.path.join(glove_folder, 'glove.6B.300d.txt')\n",
    "\n",
    "def load_glove_vectors(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        embs = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "\n",
    "            if curr_word in data_words:\n",
    "                try:\n",
    "                    embedding = np.array([float(value) for value in line[1:]])\n",
    "                    embs[curr_word] = embedding\n",
    "                except:\n",
    "                    print('error loading embedding')\n",
    "    return words, word_to_vec_map, embs\n",
    "\n",
    "glove_words, glove_word_to_vec_map, data_word_to_vec_map = load_glove_vectors(glove_file)\n",
    "# print(len(words))\n",
    "# print(len(word_to_vec_map))\n",
    "# print(word_to_vec_map['the'])\n",
    "# print(word_to_vec_map['the'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we perform a similar operation to the previous cell, but for aspect categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_catogories_glove_embedding(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        embs = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "\n",
    "            if curr_word in aspect_categories:\n",
    "                try:\n",
    "                    embedding = np.array([float(value) for value in line[1:]])\n",
    "                    embs[curr_word] = embedding\n",
    "                except:\n",
    "                    print('error loading embedding')\n",
    "    return embs\n",
    "\n",
    "aspect_catogories_to_vec_map = get_aspect_catogories_glove_embedding(glove_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generally, we are ignoring the words that are not in glove vector, we can consider the average of all encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_word_vector = np.mean(list(glove_word_to_vec_map.values()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6959\n",
      "5449\n",
      "1510\n"
     ]
    }
   ],
   "source": [
    "print(len(data_words))\n",
    "print(len(data_word_to_vec_map))\n",
    "missing_words = len(data_words) - len(data_word_to_vec_map)\n",
    "print(missing_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(aspect_categories))\n",
    "print(len(aspect_catogories_to_vec_map))\n",
    "missing_aspect_categories_words = len(aspect_catogories_to_vec_map) - len(aspect_categories)\n",
    "print(missing_aspect_categories_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = []\n",
    "idx2word = []\n",
    "word2idx = {}\n",
    "embedding_matrix.append(np.zeros(300)) # this will be our zero padding for the network\n",
    "idx2word.append('')\n",
    "word2idx[''] = 0\n",
    "for i, (word, emb) in enumerate(data_word_to_vec_map.items()):\n",
    "    embedding_matrix.append(emb)\n",
    "    idx2word.append(word)\n",
    "    word2idx[word] = i + 1\n",
    "    # word2idx[word] = i\n",
    "embedding_matrix = np.asarray(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_embedding_matrix = []\n",
    "ac_idx2word = []\n",
    "ac_word2idx = {}\n",
    "# ac_embedding_matrix.append(np.zeros(300)) # this will be our zero padding for the network\n",
    "# ac_idx2word.append('')\n",
    "# ac_word2idx[''] = 0\n",
    "for i, (word, emb) in enumerate(aspect_catogories_to_vec_map.items()):\n",
    "    ac_embedding_matrix.append(emb)\n",
    "    ac_idx2word.append(word)\n",
    "    # ac_word2idx[word] = i + 1\n",
    "    ac_word2idx[word] = i\n",
    "ac_embedding_matrix = np.asarray(ac_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "for example in sentence_data:\n",
    "    temp = []\n",
    "    for word in example.split():\n",
    "        if word[-1] in ['.',',','!','?']:\n",
    "            word = word[:-1]\n",
    "        if word in word2idx:\n",
    "            temp.append(word2idx[word])\n",
    "    # if len(temp) == 0:\n",
    "    #     print(example)\n",
    "    x_train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_train = []\n",
    "for example in aspect_data:\n",
    "    temp = []\n",
    "    for word in example.split():\n",
    "        if word[-1] in ['.',',','!','?']:\n",
    "            word = word[:-1]\n",
    "        if word in ac_word2idx:\n",
    "            temp.append(ac_word2idx[word])\n",
    "    # if len(temp) == 0:\n",
    "    #     print(example)\n",
    "    ac_train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_train = np.asarray(ac_train, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7091,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7091, 1)\n"
     ]
    }
   ],
   "source": [
    "print(ac_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "min_length = 1000\n",
    "for example in x_train:\n",
    "    if len(example) > max_length:\n",
    "        max_length = len(example)\n",
    "    if len(example) < min_length:\n",
    "        min_length = len(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(max_length)\n",
    "print(min_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.753631363700466\n"
     ]
    }
   ],
   "source": [
    "total_length = 0\n",
    "for i in range(len(x_train)):\n",
    "    total_length += len(x_train[i])\n",
    "avg_length = total_length / len(x_train)\n",
    "print(avg_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_train)):\n",
    "    x_train[i] = np.pad(x_train[i], (max_length - len(x_train[i]), 0), 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data = []\n",
    "for x in x_train:\n",
    "    x_train_data.append([k for k in x])\n",
    "\n",
    "x_train_data = np.array(x_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_train_data = []\n",
    "for x in ac_train:\n",
    "    ac_train_data.append([k for k in x])\n",
    "\n",
    "ac_train_data = np.array(ac_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5450, 300)\n",
      "5449\n",
      "6959\n",
      "(7091,)\n",
      "(7091,)\n",
      "(8, 300)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)\n",
    "print(len(data_word_to_vec_map))\n",
    "print(len(data_words))\n",
    "print(np.array(sentence_data).shape)\n",
    "print(np.array(aspect_data).shape)\n",
    "print(ac_embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7091, 64)\n",
      "(7091, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_data.shape)\n",
    "print(ac_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5450, 300)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our implementation of GCAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Gate_Aspect_Text(nn.Module):\n",
    "    def __init__(self, embedding_matrix, class_num, kernel_num, kernel_sizes, aspect_matrix):\n",
    "        super(CNN_Gate_Aspect_Text, self).__init__()\n",
    "        # self.args = args\n",
    "        \n",
    "        V = embedding_matrix.shape[0]\n",
    "        D = embedding_matrix.shape[1]\n",
    "        C = class_num\n",
    "        A = aspect_matrix.shape[0]\n",
    "\n",
    "        Co = kernel_num\n",
    "        Ks = kernel_sizes\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        self.embed.load_state_dict({'weight': torch.tensor(embedding_matrix)})\n",
    "        # self.embed.weight = nn.Parameter(embedding_matrix, requires_grad=True)\n",
    "        self.embed.weight.requires_grad = True\n",
    "\n",
    "        self.aspect_embed = nn.Embedding(A, aspect_matrix.shape[1])\n",
    "        self.aspect_embed.load_state_dict({'weight':  torch.tensor(aspect_matrix)})\n",
    "        # self.aspect_embed.weight = nn.Parameter(aspect_matrix, requires_grad=True)\n",
    "        self.aspect_embed.weight.requires_grad = True\n",
    "\n",
    "        self.convs1 = nn.ModuleList([nn.Conv1d(D, Co, K) for K in Ks])\n",
    "        self.convs2 = nn.ModuleList([nn.Conv1d(D, Co, K) for K in Ks])\n",
    "\n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C)\n",
    "        self.fc_aspect = nn.Linear(aspect_matrix.shape[1], Co)\n",
    "\n",
    "    def forward(self, feature, aspect):\n",
    "        feature = self.embed(feature) # (N, L, D)\n",
    "        aspect_v = self.aspect_embed(aspect)  # (N, L', D)\n",
    "        # aspect_v = aspect_v.sum(1) / aspect_v.size(1)\n",
    "\n",
    "        x = [F.tanh(conv(feature.transpose(1, 2))) for conv in self.convs1]  # [(N,Co,L), ...]*len(Ks)\n",
    "        y = [F.relu(conv(feature.transpose(1, 2)) + self.fc_aspect(aspect_v).unsqueeze(2)) for conv in self.convs2]\n",
    "        x = [i*j for i, j in zip(x, y)]\n",
    "\n",
    "        # pooling method\n",
    "        x0 = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N,Co), ...]*len(Ks)\n",
    "        x0 = [i.view(i.size(0), -1) for i in x0]\n",
    "\n",
    "        x0 = torch.cat(x0, 1)\n",
    "        logit = self.fc1(x0)  # (N,C)\n",
    "        return logit, x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique sentiments in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive': 4215, 'neutral': 998, 'negative': 1878}\n",
      "(7091,)\n"
     ]
    }
   ],
   "source": [
    "sentiments = {}\n",
    "\n",
    "# get unique sentiments in sentiment data\n",
    "for sentiment in sentiment_data:\n",
    "    if sentiment not in sentiments:\n",
    "        sentiments[sentiment] = 1\n",
    "    else:\n",
    "        sentiments[sentiment] += 1\n",
    "\n",
    "print(sentiments)\n",
    "\n",
    "sentiment_input = []\n",
    "for sentiment in sentiment_data:\n",
    "    if sentiment == 'positive':\n",
    "        sentiment_input.append(2)\n",
    "    elif sentiment == 'negative':\n",
    "        sentiment_input.append(0)\n",
    "    else:\n",
    "        sentiment_input.append(1)\n",
    "\n",
    "sentiment_input = np.array(sentiment_input)\n",
    "print(sentiment_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = CustomImageDataset(x_train_data, labels)\n",
    "\n",
    "train_length = int(len(sentence_data) * 0.8) # 80% training data, 20% test data\n",
    "test_length = len(sentence_data) - train_length\n",
    "\n",
    "# print(x_train_data.shape)\n",
    "\n",
    "# concatenate the x_train_data, ac_train_data and sentiment_input\n",
    "x_train_data = np.concatenate((x_train_data, ac_train_data), axis=1)\n",
    "# print(x_train_data.shape)\n",
    "x_train_data = np.concatenate((x_train_data, sentiment_input.reshape(-1, 1)), axis=1)\n",
    "\n",
    "# print(x_train_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "# print(len(x_train_dataloader) * batch_size)\n",
    "# print(len(y_test_dataloader) * batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train and test data into 0.8, 0.2 sizes respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5672, 66)\n"
     ]
    }
   ],
   "source": [
    "# split x_train_data into training and test data using train_test_split\n",
    "x_train, x_test = train_test_split(x_train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader of batch size 32 of both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# x_train = torch.tensor(x_train.astype('float64')).to(torch.int64)\n",
    "train_batches = DataLoader(torch.Tensor(x_train).to(dtype=torch.long), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# for data in train_batches:\n",
    "#     # print(data)\n",
    "#     # break\n",
    "# # convert data to int tensor\n",
    "#     # data = data.to(torch.int64)\n",
    "#     print(data.shape)\n",
    "#     print(data[:, :-2].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = DataLoader(torch.Tensor(x_test).to(dtype=torch.long), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_string(tokens):  # Convert tokens back into their sting value\n",
    "    words = [idx2word[token] for token in tokens]\n",
    "    text = \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ac_tokens_to_string(tokens):  # Convert tokens back into their sting value\n",
    "    words = [ac_idx2word[tokens]]\n",
    "    text = \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our model with 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srisa\\AppData\\Local\\Temp\\ipykernel_19664\\2460389490.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embed.load_state_dict({'weight': torch.tensor(embedding_matrix)})\n",
      "C:\\Users\\srisa\\AppData\\Local\\Temp\\ipykernel_19664\\2460389490.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.aspect_embed.load_state_dict({'weight':  torch.tensor(aspect_matrix)})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Batch:  0 Loss:  1.080554723739624\n",
      "Epoch:  0 Batch:  100 Loss:  0.6939608454704285\n",
      "Epoch:  1 Batch:  0 Loss:  0.6040027141571045\n",
      "Epoch:  1 Batch:  100 Loss:  0.2938236892223358\n",
      "Epoch:  2 Batch:  0 Loss:  0.3107961416244507\n",
      "Epoch:  2 Batch:  100 Loss:  0.2827276587486267\n",
      "Epoch:  3 Batch:  0 Loss:  0.0591704286634922\n",
      "Epoch:  3 Batch:  100 Loss:  0.46321162581443787\n",
      "Epoch:  4 Batch:  0 Loss:  0.17658980190753937\n",
      "Epoch:  4 Batch:  100 Loss:  0.13112111389636993\n",
      "Epoch:  5 Batch:  0 Loss:  0.07263714075088501\n",
      "Epoch:  5 Batch:  100 Loss:  0.012355015613138676\n",
      "Epoch:  6 Batch:  0 Loss:  0.0626915916800499\n",
      "Epoch:  6 Batch:  100 Loss:  0.08076046407222748\n",
      "Epoch:  7 Batch:  0 Loss:  0.012353479862213135\n",
      "Epoch:  7 Batch:  100 Loss:  0.007546519860625267\n",
      "Epoch:  8 Batch:  0 Loss:  0.007602863945066929\n",
      "Epoch:  8 Batch:  100 Loss:  0.04219546541571617\n",
      "Epoch:  9 Batch:  0 Loss:  0.0049159228801727295\n",
      "Epoch:  9 Batch:  100 Loss:  0.003593805944547057\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    model = CNN_Gate_Aspect_Text(torch.Tensor(embedding_matrix).to(dtype=torch.long), len(sentiments), kernel_num, kernel_vec, torch.Tensor(ac_embedding_matrix).to(dtype=torch.long))\n",
    "    # model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, data in enumerate(train_batches):\n",
    "            sentence = data[:, :-2]\n",
    "            aspect = data[:, -2]\n",
    "            sentiment = data[:, -1]\n",
    "\n",
    "            # print(sentence.shape)\n",
    "            # print(aspect.shape)\n",
    "            # print(sentiment.shape)\n",
    "\n",
    "            # for i in range(len(sentence)):\n",
    "            #     print(tokens_to_string(sentence[i].to(dtype=torch.long, device='cpu')))\n",
    "            #     print(ac_idx2word[aspect[i].to(dtype=torch.long, device='cpu')])\n",
    "            #     print(sentiment[i])\n",
    "\n",
    "            # print(aspect[0])\n",
    "\n",
    "            # x = x.to(device)\n",
    "            # y = y.to(device)\n",
    "            # optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            # model.zero_grad()\n",
    "            # convert sentence to int tensor\n",
    "            sentence = sentence.to(dtype=torch.long)\n",
    "            aspect = aspect.to(dtype=torch.long)\n",
    "            \n",
    "            logit, x, y = model(sentence, aspect)\n",
    "            # y = torch.tensor(y)\n",
    "            loss = loss_function(logit, sentiment)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('Epoch: ', epoch, 'Batch: ', i, 'Loss: ', loss.item())\n",
    "            \n",
    "    return model\n",
    "\n",
    "model = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are calculating score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "\n",
    "def print_accuracy(dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    visual = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            sentence = data[:, :-2]\n",
    "            aspect = data[:, -2]\n",
    "            sentiment = data[:, -1]\n",
    "            outputs, _, _ = model(sentence, aspect)\n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            sentiment = sentiment.detach().cpu().numpy()\n",
    "            # output the labels with highest probability\n",
    "            outputs = np.argmax(outputs, axis=1)\n",
    "            # print(outputs)\n",
    "            # labels = labels == 1.0\n",
    "            # use outputs to calculate accuracy\n",
    "            correct += np.sum(outputs == sentiment)\n",
    "            total += len(outputs)\n",
    "            for i in range(len(sentence)):\n",
    "                visual.append([tokens_to_string(sentence[i].tolist()), ac_tokens_to_string(aspect[i]), sentiment[i], outputs[i]])\n",
    "                \n",
    "    # Dump each list in visual data as a row in a csv file\n",
    "    with open('visual.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(visual) \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accuracy on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9948871650211566\n"
     ]
    }
   ],
   "source": [
    "print(print_accuracy(train_batches))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7575757575757576\n"
     ]
    }
   ],
   "source": [
    "print(print_accuracy(test_batches))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "063993497d7afc17216208e6d2fa098ad08ff0cdca94ee4cdde88ee1996574e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
