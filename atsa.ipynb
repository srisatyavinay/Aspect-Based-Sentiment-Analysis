{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.utils import shuffle\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_num = 100\n",
    "kernel_vec = [3, 4, 5]\n",
    "epochs = 10\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are using Restaurant-large dataset.\n",
    "* Here we are loading the train and test datasets.\n",
    "* Also we are extracting sentences, aspects and sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file into a list\n",
    "with open('./atsa-restaurant/atsa_train.json','rb') as f:\n",
    "    data1 = json.load(f)\n",
    "\n",
    "with open('./atsa-restaurant/atsa_test.json','rb') as f:\n",
    "    data2 = json.load(f)\n",
    "\n",
    "sentence_data = [x['sentence'] for x in data1] + [x['sentence'] for x in data2]\n",
    "aspect_data = [x['aspect'] for x in data1] + [x['aspect'] for x in data2]\n",
    "sentiment_data = [x['sentiment'] for x in data1] + [x['sentiment'] for x in data2]\n",
    "\n",
    "sentence_data, aspect_data, sentiment_data = shuffle(sentence_data, aspect_data, sentiment_data)\n",
    "\n",
    "# print('Number of sentences: ', len(sentence_data))\n",
    "# print('Number of aspects: ', len(aspect_data))\n",
    "# print('Number of sentiments: ', len(sentiment_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the following cell, we are just making a wordcount of each word in the sentences\n",
    "* We are removing symbols at end of each word.\n",
    "* If the word occurs for the first time, we assign the count to zero\n",
    "* else we increase the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = {}\n",
    "for example in sentence_data:\n",
    "    for word in example.split():\n",
    "        if word[-1] in ['.',',','!','?']:\n",
    "            word = word[:-1]\n",
    "        if word not in data_words:\n",
    "            data_words[word] = 0\n",
    "        else:\n",
    "            data_words[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Similar to the above cell, we perform the same operations for aspect categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_categories = {}\n",
    "for example in aspect_data:\n",
    "    for word in example.split():\n",
    "        if word[-1] in ['.',',','!','?']:\n",
    "            word = word[:-1]\n",
    "        if word not in aspect_categories:\n",
    "            aspect_categories[word] = 0\n",
    "        else:\n",
    "            aspect_categories[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4818\n",
      "1387\n"
     ]
    }
   ],
   "source": [
    "print(len(data_words))\n",
    "print(len(aspect_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the following cell we are loading the glove file and encoding vectors for words present in our data.\n",
    "* `glove_word_to_vec_map:` All words in glove file and the corresponding encodings, \n",
    "* `data_word_to_vec_map:`All words in our data and the corresponding encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load glove vectors\n",
    "glove_folder = os.path.join(os.getcwd(), 'glove_file')\n",
    "\n",
    "# get path of glove.6B.300d.txt file in test folder\n",
    "glove_file = os.path.join(glove_folder, 'glove.6B.300d.txt')\n",
    "\n",
    "def load_glove_vectors(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        embs = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "\n",
    "            if curr_word in data_words:\n",
    "                try:\n",
    "                    embedding = np.array([float(value) for value in line[1:]])\n",
    "                    embs[curr_word] = embedding\n",
    "                except:\n",
    "                    print('error loading embedding')\n",
    "    return words, word_to_vec_map, embs\n",
    "\n",
    "glove_words, glove_word_to_vec_map, data_word_to_vec_map = load_glove_vectors(glove_file)\n",
    "# print(len(words))\n",
    "# print(len(word_to_vec_map))\n",
    "# print(word_to_vec_map['the'])\n",
    "# print(word_to_vec_map['the'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_catogories_glove_embedding(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        embs = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "\n",
    "            if curr_word in aspect_categories:\n",
    "                try:\n",
    "                    embedding = np.array([float(value) for value in line[1:]])\n",
    "                    embs[curr_word] = embedding\n",
    "                except:\n",
    "                    print('error loading embedding')\n",
    "    return embs\n",
    "\n",
    "aspect_catogories_to_vec_map = get_aspect_catogories_glove_embedding(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_word_vector = np.mean(list(glove_word_to_vec_map.values()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4818\n",
      "4044\n",
      "774\n"
     ]
    }
   ],
   "source": [
    "print(len(data_words))\n",
    "print(len(data_word_to_vec_map))\n",
    "missing_words = len(data_words) - len(data_word_to_vec_map)\n",
    "print(missing_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387\n",
      "1253\n",
      "-134\n"
     ]
    }
   ],
   "source": [
    "print(len(aspect_categories))\n",
    "print(len(aspect_catogories_to_vec_map))\n",
    "missing_aspect_categories_words = len(aspect_catogories_to_vec_map) - len(aspect_categories)\n",
    "print(missing_aspect_categories_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = []\n",
    "idx2word = []\n",
    "word2idx = {}\n",
    "embedding_matrix.append(np.zeros(300)) # this will be our zero padding for the network\n",
    "idx2word.append('')\n",
    "word2idx[''] = 0\n",
    "for i, (word, emb) in enumerate(data_word_to_vec_map.items()):\n",
    "    embedding_matrix.append(emb)\n",
    "    idx2word.append(word)\n",
    "    word2idx[word] = i + 1\n",
    "    # word2idx[word] = i\n",
    "embedding_matrix = np.asarray(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_embedding_matrix = []\n",
    "ac_idx2word = []\n",
    "ac_word2idx = {}\n",
    "# ac_embedding_matrix.append(np.zeros(300)) # this will be our zero padding for the network\n",
    "# ac_idx2word.append('')\n",
    "# ac_word2idx[''] = 0\n",
    "for i, (word, emb) in enumerate(aspect_catogories_to_vec_map.items()):\n",
    "    ac_embedding_matrix.append(emb)\n",
    "    ac_idx2word.append(word)\n",
    "    # ac_word2idx[word] = i + 1\n",
    "    ac_word2idx[word] = i\n",
    "ac_embedding_matrix = np.asarray(ac_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "for example in sentence_data:\n",
    "    temp = []\n",
    "    for word in example.split():\n",
    "        if word[-1] in ['.',',','!','?']:\n",
    "            word = word[:-1]\n",
    "        if word in word2idx:\n",
    "            temp.append(word2idx[word])\n",
    "    # if len(temp) == 0:\n",
    "    #     print(example)\n",
    "    x_train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_train = []\n",
    "for example in aspect_data:\n",
    "    temp = []\n",
    "    for word in example.split():\n",
    "        if word[-1] in ['.',',','!','?']:\n",
    "            word = word[:-1]\n",
    "        if word in ac_word2idx:\n",
    "            temp.append(ac_word2idx[word])\n",
    "    # if len(temp) == 0:\n",
    "    #     print(example)\n",
    "    ac_train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_train = np.asarray(ac_train, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4827,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4827,)\n"
     ]
    }
   ],
   "source": [
    "print(ac_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "min_length = 1000\n",
    "for example in x_train:\n",
    "    if len(example) > max_length:\n",
    "        max_length = len(example)\n",
    "    if len(example) < min_length:\n",
    "        min_length = len(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(max_length)\n",
    "print(min_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_max_length = 0\n",
    "a_min_length = 1000\n",
    "for example in ac_train:\n",
    "    if len(example) > a_max_length:\n",
    "        a_max_length = len(example)\n",
    "    if len(example) < a_min_length:\n",
    "        a_min_length = len(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(a_max_length)\n",
    "print(a_min_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.64284234514191\n"
     ]
    }
   ],
   "source": [
    "total_length = 0\n",
    "for i in range(len(x_train)):\n",
    "    total_length += len(x_train[i])\n",
    "avg_length = total_length / len(x_train)\n",
    "print(avg_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3706235757199088\n"
     ]
    }
   ],
   "source": [
    "a_total_length = 0\n",
    "for i in range(len(ac_train)):\n",
    "    a_total_length += len(ac_train[i])\n",
    "a_avg_length = a_total_length / len(ac_train)\n",
    "print(a_avg_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_train)):\n",
    "    x_train[i] = np.pad(x_train[i], (max_length - len(x_train[i]), 0), 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ac_train)):\n",
    "    ac_train[i] = np.pad(ac_train[i], (a_max_length - len(ac_train[i]), 0), 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data = []\n",
    "for x in x_train:\n",
    "    x_train_data.append([k for k in x])\n",
    "\n",
    "x_train_data = np.array(x_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_train_data = []\n",
    "for x in ac_train:\n",
    "    ac_train_data.append([k for k in x])\n",
    "\n",
    "ac_train_data = np.array(ac_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4045, 300)\n",
      "4044\n",
      "4818\n",
      "(4827,)\n",
      "(4827,)\n",
      "(1253, 300)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)\n",
    "print(len(data_word_to_vec_map))\n",
    "print(len(data_words))\n",
    "print(np.array(sentence_data).shape)\n",
    "print(np.array(aspect_data).shape)\n",
    "print(ac_embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4827, 64)\n",
      "(4827, 16)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_data.shape)\n",
    "print(ac_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4045, 300)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Gate_Aspect_Text(nn.Module):\n",
    "    def __init__(self, embedding_matrix, class_num, kernel_num, kernel_sizes, aspect_matrix):\n",
    "        super(CNN_Gate_Aspect_Text, self).__init__()\n",
    "        \n",
    "        V = embedding_matrix.shape[0]\n",
    "        D = embedding_matrix.shape[1]\n",
    "        C = class_num\n",
    "        A = aspect_matrix.shape[0]\n",
    "\n",
    "        Co = kernel_num\n",
    "        Ks = kernel_sizes\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        self.embed.load_state_dict({'weight': torch.tensor(embedding_matrix)})\n",
    "        # self.embed.weight = nn.Parameter(embedding_matrix, requires_grad=True)\n",
    "        self.embed.weight.requires_grad = True\n",
    "\n",
    "        self.aspect_embed = nn.Embedding(A, aspect_matrix.shape[1])\n",
    "        self.aspect_embed.load_state_dict({'weight':  torch.tensor(aspect_matrix)})\n",
    "        # self.aspect_embed.weight = nn.Parameter(aspect_matrix, requires_grad=True)\n",
    "        self.aspect_embed.weight.requires_grad = True\n",
    "\n",
    "        self.convs1 = nn.ModuleList([nn.Conv1d(D, Co, K) for K in Ks])\n",
    "        self.convs2 = nn.ModuleList([nn.Conv1d(D, Co, K) for K in Ks])\n",
    "        self.convs3 = nn.ModuleList([nn.Conv1d(D, Co, K, padding=K-2) for K in [3]])\n",
    "\n",
    "\n",
    "        # self.convs3 = nn.Conv1d(D, 300, 3, padding=1), smaller is better\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C)\n",
    "        self.fc_aspect = nn.Linear(100, Co)\n",
    "\n",
    "\n",
    "    def forward(self, feature, aspect):\n",
    "        feature = self.embed(feature)  # (N, L, D)\n",
    "        aspect_v = self.aspect_embed(aspect)  # (N, L', D)\n",
    "        aa = [F.relu(conv(aspect_v.transpose(1, 2))) for conv in self.convs3]  # [(N,Co,L), ...]*len(Ks)\n",
    "        aa = [F.max_pool1d(a, a.size(2)).squeeze(2) for a in aa]\n",
    "        aspect_v = torch.cat(aa, 1)\n",
    "        # aa = F.tanhshrink(self.convs3(aspect_v.transpose(1, 2)))  # [(N,Co,L), ...]*len(Ks)\n",
    "        # aa = F.max_pool1d(aa, aa.size(2)).squeeze(2)\n",
    "        # aspect_v = aa\n",
    "        # smaller is better\n",
    "\n",
    "        x = [F.tanh(conv(feature.transpose(1, 2))) for conv in self.convs1]  # [(N,Co,L), ...]*len(Ks)\n",
    "        y = [F.relu(conv(feature.transpose(1, 2)) + self.fc_aspect(aspect_v).unsqueeze(2)) for conv in self.convs2]\n",
    "        x = [i*j for i, j in zip(x, y)]\n",
    "\n",
    "        # pooling method\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N,Co), ...]*len(Ks)\n",
    "        # x = [F.adaptive_max_pool1d(i, 2) for i in x]\n",
    "        # x = [i.view(i.size(0), -1) for i in x]\n",
    "\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)  # (N,len(Ks)*Co)\n",
    "        logit = self.fc1(x)  # (N,C)\n",
    "        return logit, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neutral': 829, 'negative': 1001, 'positive': 2892, 'conflict': 105}\n",
      "(4827,)\n"
     ]
    }
   ],
   "source": [
    "sentiments = {}\n",
    "\n",
    "# get unique sentiments in sentiment data\n",
    "for sentiment in sentiment_data:\n",
    "    if sentiment not in sentiments:\n",
    "        sentiments[sentiment] = 1\n",
    "    else:\n",
    "        sentiments[sentiment] += 1\n",
    "\n",
    "print(sentiments)\n",
    "\n",
    "sentiment_input = []\n",
    "for sentiment in sentiment_data:\n",
    "    if sentiment == 'positive':\n",
    "        sentiment_input.append(2)\n",
    "    elif sentiment == 'negative':\n",
    "        sentiment_input.append(0)\n",
    "    else:\n",
    "        sentiment_input.append(1)\n",
    "\n",
    "sentiment_input = np.array(sentiment_input)\n",
    "print(sentiment_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = CustomImageDataset(x_train_data, labels)\n",
    "\n",
    "train_length = int(len(sentence_data) * 0.8) # 80% training data, 20% test data\n",
    "test_length = len(sentence_data) - train_length\n",
    "\n",
    "# print(x_train_data.shape)\n",
    "\n",
    "# concatenate the x_train_data, ac_train_data and sentiment_input\n",
    "x_train_data = np.concatenate((x_train_data, ac_train_data), axis=1)\n",
    "# print(x_train_data.shape)\n",
    "x_train_data = np.concatenate((x_train_data, sentiment_input.reshape(-1, 1)), axis=1)\n",
    "\n",
    "# print(x_train_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "# print(len(x_train_dataloader) * batch_size)\n",
    "# print(len(y_test_dataloader) * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3861, 81)\n"
     ]
    }
   ],
   "source": [
    "# split x_train_data into training and test data using train_test_split\n",
    "x_train, x_test = train_test_split(x_train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# x_train = torch.tensor(x_train.astype('float64')).to(torch.int64)\n",
    "train_batches = DataLoader(torch.Tensor(x_train).to(dtype=torch.long), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# for data in train_batches:\n",
    "#     # print(data)\n",
    "#     # break\n",
    "# # convert data to int tensor\n",
    "#     # data = data.to(torch.int64)\n",
    "#     print(data.shape)\n",
    "#     print(data[:, :-2].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = DataLoader(torch.Tensor(x_test).to(dtype=torch.long), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_string(tokens):  # Convert tokens back into their sting value\n",
    "    words = [idx2word[token] for token in tokens]\n",
    "    text = \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ac_tokens_to_string(tokens):  # Convert tokens back into their sting value\n",
    "    words = [ac_idx2word[token] for token in tokens]\n",
    "    text = \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srisa\\AppData\\Local\\Temp\\ipykernel_7528\\3799450476.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embed.load_state_dict({'weight': torch.tensor(embedding_matrix)})\n",
      "C:\\Users\\srisa\\AppData\\Local\\Temp\\ipykernel_7528\\3799450476.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.aspect_embed.load_state_dict({'weight':  torch.tensor(aspect_matrix)})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Batch:  0 Loss:  1.0995526313781738\n",
      "Epoch:  0 Batch:  100 Loss:  0.7206839323043823\n",
      "Epoch:  1 Batch:  0 Loss:  0.7448155879974365\n",
      "Epoch:  1 Batch:  100 Loss:  0.31195175647735596\n",
      "Epoch:  2 Batch:  0 Loss:  0.3568362295627594\n",
      "Epoch:  2 Batch:  100 Loss:  0.420025497674942\n",
      "Epoch:  3 Batch:  0 Loss:  0.23110097646713257\n",
      "Epoch:  3 Batch:  100 Loss:  0.32050710916519165\n",
      "Epoch:  4 Batch:  0 Loss:  0.220163956284523\n",
      "Epoch:  4 Batch:  100 Loss:  0.20904314517974854\n",
      "Epoch:  5 Batch:  0 Loss:  0.15926630795001984\n",
      "Epoch:  5 Batch:  100 Loss:  0.16802304983139038\n",
      "Epoch:  6 Batch:  0 Loss:  0.021402699872851372\n",
      "Epoch:  6 Batch:  100 Loss:  0.20569710433483124\n",
      "Epoch:  7 Batch:  0 Loss:  0.11067144572734833\n",
      "Epoch:  7 Batch:  100 Loss:  0.14777164161205292\n",
      "Epoch:  8 Batch:  0 Loss:  0.04526834562420845\n",
      "Epoch:  8 Batch:  100 Loss:  0.041393280029296875\n",
      "Epoch:  9 Batch:  0 Loss:  0.040724147111177444\n",
      "Epoch:  9 Batch:  100 Loss:  0.01950805075466633\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    model = CNN_Gate_Aspect_Text(torch.Tensor(embedding_matrix).to(dtype=torch.long), len(sentiments), kernel_num, kernel_vec, torch.Tensor(ac_embedding_matrix).to(dtype=torch.long))\n",
    "    # model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, data in enumerate(train_batches):\n",
    "            sentence = data[:, :-(a_max_length+1)]\n",
    "            aspect = data[:, -(a_max_length+1):-1]\n",
    "            sentiment = data[:, -1]\n",
    "\n",
    "            # print(sentence.shape)\n",
    "            # print(aspect.shape)\n",
    "            # print(sentiment.shape)\n",
    "\n",
    "            # for i in range(len(sentence)):\n",
    "            #     print(tokens_to_string(sentence[i].to(dtype=torch.long, device='cpu')))\n",
    "            #     print(ac_idx2word[aspect[i].to(dtype=torch.long, device='cpu')])\n",
    "            #     print(sentiment[i])\n",
    "\n",
    "            # print(aspect[0])\n",
    "\n",
    "            # x = x.to(device)\n",
    "            # y = y.to(device)\n",
    "            # optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            # model.zero_grad()\n",
    "            # convert sentence to int tensor\n",
    "            sentence = sentence.to(dtype=torch.long)\n",
    "            aspect = aspect.to(dtype=torch.long)\n",
    "            \n",
    "            logit, x, y = model(sentence, aspect)\n",
    "            # y = torch.tensor(y)\n",
    "            loss = loss_function(logit, sentiment)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('Epoch: ', epoch, 'Batch: ', i, 'Loss: ', loss.item())\n",
    "            \n",
    "    return model\n",
    "\n",
    "model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "\n",
    "def print_accuracy(dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    visual =[]\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            sentence = data[:, :-(a_max_length+1)]\n",
    "            aspect = data[:, -(a_max_length+1):-1]\n",
    "            sentiment = data[:, -1]\n",
    "            outputs, _, _ = model(sentence, aspect)\n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            sentiment = sentiment.detach().cpu().numpy()\n",
    "            # output the labels with highest probability\n",
    "            outputs = np.argmax(outputs, axis=1)\n",
    "            # print(outputs)\n",
    "            # labels = labels == 1.0\n",
    "            # use outputs to calculate accuracy\n",
    "            correct += np.sum(outputs == sentiment)\n",
    "            total += len(outputs)\n",
    "            for i in range(len(sentence)):\n",
    "                visual.append([tokens_to_string(sentence[i].tolist()), ac_tokens_to_string(aspect[i].tolist()), sentiment[i], outputs[i]])\n",
    "    with open('visual.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(visual)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9849779849779849\n"
     ]
    }
   ],
   "source": [
    "print(print_accuracy(train_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.727743271221532\n"
     ]
    }
   ],
   "source": [
    "print(print_accuracy(test_batches))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "063993497d7afc17216208e6d2fa098ad08ff0cdca94ee4cdde88ee1996574e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
