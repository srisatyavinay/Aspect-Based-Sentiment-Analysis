{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file into a list\n",
    "with open('./acsa-restaurant-large/acsa_train.json','rb') as f:\n",
    "    data1 = json.load(f)\n",
    "\n",
    "with open('./acsa-restaurant-large/acsa_test.json','rb') as f:\n",
    "    data2 = json.load(f)\n",
    "\n",
    "sentence_data = [x['sentence'] for x in data1] + [x['sentence'] for x in data2]\n",
    "aspect_data = [x['aspect'] for x in data1] + [x['aspect'] for x in data2]\n",
    "sentiment_data = [x['sentiment'] for x in data1] + [x['sentiment'] for x in data2]\n",
    "\n",
    "sentence_data, aspect_data, sentiment_data = shuffle(sentence_data, aspect_data, sentiment_data)\n",
    "\n",
    "# print('Number of sentences: ', len(sentence_data))\n",
    "# print('Number of aspects: ', len(aspect_data))\n",
    "# print('Number of sentiments: ', len(sentiment_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = {}\n",
    "for example in sentence_data:\n",
    "    for word in example.split():\n",
    "        if word[-1] in ['.',',','!','?']:\n",
    "            word = word[:-1]\n",
    "        if word not in data_words:\n",
    "            data_words[word] = 0\n",
    "        else:\n",
    "            data_words[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_categories = {}\n",
    "for example in aspect_data:\n",
    "    for word in example.split():\n",
    "        if word[-1] in ['.',',','!','?']:\n",
    "            word = word[:-1]\n",
    "        if word not in aspect_categories:\n",
    "            aspect_categories[word] = 0\n",
    "        else:\n",
    "            aspect_categories[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6959\n"
     ]
    }
   ],
   "source": [
    "# print(len(data_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load glove vectors\n",
    "glove_folder = os.path.join(os.getcwd(), 'glove_file')\n",
    "\n",
    "# get path of glove.6B.300d.txt file in test folder\n",
    "glove_file = os.path.join(glove_folder, 'glove.6B.300d.txt')\n",
    "\n",
    "def load_glove_vectors(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        embs = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "\n",
    "            if curr_word in data_words:\n",
    "                try:\n",
    "                    embedding = np.array([float(value) for value in line[1:]])\n",
    "                    embs[curr_word] = embedding\n",
    "                except:\n",
    "                    print('error loading embedding')\n",
    "    return words, word_to_vec_map, embs\n",
    "\n",
    "glove_words, glove_word_to_vec_map, data_word_to_vec_map = load_glove_vectors(glove_file)\n",
    "# print(len(words))\n",
    "# print(len(word_to_vec_map))\n",
    "# print(word_to_vec_map['the'])\n",
    "# print(word_to_vec_map['the'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_catogories_glove_embedding(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        embs = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "\n",
    "            if curr_word in aspect_categories:\n",
    "                try:\n",
    "                    embedding = np.array([float(value) for value in line[1:]])\n",
    "                    embs[curr_word] = embedding\n",
    "                except:\n",
    "                    print('error loading embedding')\n",
    "    return embs\n",
    "\n",
    "aspect_catogories_to_vec_map = get_aspect_catogories_glove_embedding(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_word_vector = np.mean(list(glove_word_to_vec_map.values()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6959\n",
      "5449\n",
      "1510\n"
     ]
    }
   ],
   "source": [
    "# print(len(data_words))\n",
    "# print(len(data_word_to_vec_map))\n",
    "missing_words = len(data_words) - len(data_word_to_vec_map)\n",
    "# print(missing_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# print(len(aspect_categories))\n",
    "# print(len(aspect_catogories_to_vec_map))\n",
    "missing_aspect_categories_words = len(aspect_catogories_to_vec_map) - len(aspect_categories)\n",
    "# print(missing_aspect_categories_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = []\n",
    "idx2word = []\n",
    "word2idx = {}\n",
    "embedding_matrix.append(np.zeros(300)) # this will be our zero padding for the network\n",
    "idx2word.append('')\n",
    "word2idx[''] = 0\n",
    "for i, (word, emb) in enumerate(data_word_to_vec_map.items()):\n",
    "    embedding_matrix.append(emb)\n",
    "    idx2word.append(word)\n",
    "    word2idx[word] = i + 1\n",
    "    # word2idx[word] = i\n",
    "embedding_matrix = np.asarray(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_embedding_matrix = []\n",
    "ac_idx2word = []\n",
    "ac_word2idx = {}\n",
    "# ac_embedding_matrix.append(np.zeros(300)) # this will be our zero padding for the network\n",
    "# ac_idx2word.append('')\n",
    "# ac_word2idx[''] = 0\n",
    "for i, (word, emb) in enumerate(aspect_catogories_to_vec_map.items()):\n",
    "    ac_embedding_matrix.append(emb)\n",
    "    ac_idx2word.append(word)\n",
    "    # ac_word2idx[word] = i + 1\n",
    "    ac_word2idx[word] = i\n",
    "ac_embedding_matrix = np.asarray(ac_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "for example in sentence_data:\n",
    "    temp = []\n",
    "    for word in example.split():\n",
    "        if word[-1] in ['.',',','!','?']:\n",
    "            word = word[:-1]\n",
    "        if word in word2idx:\n",
    "            temp.append(word2idx[word])\n",
    "    # if len(temp) == 0:\n",
    "    #     print(example)\n",
    "    x_train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_train = []\n",
    "for example in aspect_data:\n",
    "    temp = []\n",
    "    for word in example.split():\n",
    "        if word[-1] in ['.',',','!','?']:\n",
    "            word = word[:-1]\n",
    "        if word in ac_word2idx:\n",
    "            temp.append(ac_word2idx[word])\n",
    "    # if len(temp) == 0:\n",
    "    #     print(example)\n",
    "    ac_train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_train = np.asarray(ac_train, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ac_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "min_length = 1000\n",
    "for example in x_train:\n",
    "    if len(example) > max_length:\n",
    "        max_length = len(example)\n",
    "    if len(example) < min_length:\n",
    "        min_length = len(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(max_length)\n",
    "print(min_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.753631363700466\n"
     ]
    }
   ],
   "source": [
    "total_length = 0\n",
    "for i in range(len(x_train)):\n",
    "    total_length += len(x_train[i])\n",
    "avg_length = total_length / len(x_train)\n",
    "print(avg_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_train)):\n",
    "    x_train[i] = np.pad(x_train[i], (max_length - len(x_train[i]), 0), 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data = []\n",
    "for x in x_train:\n",
    "    x_train_data.append([k for k in x])\n",
    "\n",
    "x_train_data = np.array(x_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_train_data = []\n",
    "for x in ac_train:\n",
    "    ac_train_data.append([k for k in x])\n",
    "\n",
    "ac_train_data = np.array(ac_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5450, 300)\n",
      "5449\n",
      "6959\n",
      "(7091,)\n"
     ]
    }
   ],
   "source": [
    "# print(embedding_matrix.shape)\n",
    "# print(len(data_word_to_vec_map))\n",
    "# print(len(data_words))\n",
    "# print(np.array(sentence_data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train_data.shape)\n",
    "# print(ac_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Gate_Aspect_Text(nn.Module):\n",
    "    def __init__(self, embedding_matrix, class_num, kernel_num, kernel_sizes, aspect_matrix):\n",
    "        super(CNN_Gate_Aspect_Text, self).__init__()\n",
    "        # self.args = args\n",
    "        \n",
    "        V = embedding_matrix.shape[0]\n",
    "        D = embedding_matrix.shape[1]\n",
    "        C = class_num\n",
    "        A = aspect_matrix.shape[0]\n",
    "\n",
    "        Co = kernel_num\n",
    "        Ks = kernel_sizes\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        self.embed.weight = nn.Parameter(embedding_matrix, requires_grad=True)\n",
    "\n",
    "        self.aspect_embed = nn.Embedding(A, aspect_matrix.shape[1])\n",
    "        self.aspect_embed.weight = nn.Parameter(aspect_matrix, requires_grad=True)\n",
    "\n",
    "        self.convs1 = nn.ModuleList([nn.Conv1d(D, Co, K) for K in Ks])\n",
    "        self.convs2 = nn.ModuleList([nn.Conv1d(D, Co, K) for K in Ks])\n",
    "\n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C)\n",
    "        self.fc_aspect = nn.Linear(aspect_matrix.shape[1], Co)\n",
    "\n",
    "    def forward(self, feature, aspect):\n",
    "        feature = self.embed(feature)  # (N, L, D)\n",
    "        aspect_v = self.aspect_embed(aspect)  # (N, L', D)\n",
    "        aspect_v = aspect_v.sum(1) / aspect_v.size(1)\n",
    "\n",
    "        x = [F.tanh(conv(feature.transpose(1, 2))) for conv in self.convs1]  # [(N,Co,L), ...]*len(Ks)\n",
    "        y = [F.relu(conv(feature.transpose(1, 2)) + self.fc_aspect(aspect_v).unsqueeze(2)) for conv in self.convs2]\n",
    "        x = [i*j for i, j in zip(x, y)]\n",
    "\n",
    "        # pooling method\n",
    "        x0 = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N,Co), ...]*len(Ks)\n",
    "        x0 = [i.view(i.size(0), -1) for i in x0]\n",
    "\n",
    "        x0 = torch.cat(x0, 1)\n",
    "        logit = self.fc1(x0)  # (N,C)\n",
    "        return logit, x, y\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "063993497d7afc17216208e6d2fa098ad08ff0cdca94ee4cdde88ee1996574e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
